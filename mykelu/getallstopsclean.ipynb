{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing OpenTransit Bus Location Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to present an example of working with the provided bus location data. We'll take a subset of this data (pulled only from buses on routes 1 and 14), along with geographical information about the stops on each route, to find every instance in our data set of a bus arriving at a stop along its route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from geopy.distance import distance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename = 'routes24h20181015v2.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_locations = load_json('route_14_week_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Stop and Bus IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to filter the location data so we're left with every instance of a bus arriving at a bus stop. We'll extract the location data for every stop on a given route (the current implementation was designed with finding these instances on a given route for a short time period). We do the same for bus locations on a given route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_stops(data: list, route: str) -> pd.DataFrame:\n",
    "    stops = pd.io.json.json_normalize(data,\n",
    "                                      record_path=['stops']) \\\n",
    "            .rename(columns={'lat': 'LAT',\n",
    "                             'lon': 'LON',\n",
    "                             'sid': 'SID'}) \\\n",
    "            .reindex(['SID', 'LAT', 'LON'], axis='columns')\n",
    "    \n",
    "    # obtain stop directions\n",
    "    stops['DID'] = stops['SID'].map({stop: direction['id']\n",
    "                                     for direction in requests\n",
    "                                                      .get(f\"http://restbus.info/api/agencies/sf-muni/routes/{route}\")\n",
    "                                                      .json()['directions']\n",
    "                                     for stop in direction['stops']})\n",
    "    \n",
    "    # remove stops that don't have an associated direction\n",
    "    stops = stops.dropna(axis='index', subset=['DID'])\n",
    "    request = requests.get(f\"http://restbus.info/api/agencies/sf-muni/routes/{route}\")\n",
    "\n",
    "    # obtain stop ordinals\n",
    "    stops['ORD'] = stops['SID'].map({stop_meta['id']: ordinal\n",
    "                                     for ordinal, stop_meta\n",
    "                                     in enumerate(request.json()['stops'])})\n",
    "    \n",
    "    return stops\n",
    "\n",
    "def produce_buses(data: list) -> pd.DataFrame:\n",
    "     return pd.io.json.json_normalize(data,\n",
    "                                      record_path=['routeStates', 'vehicles'],\n",
    "                                      meta=[['routeStates', 'vtime']]) \\\n",
    "            .rename(columns={'lat': 'LAT',\n",
    "                             'lon': 'LON',\n",
    "                             'vid': 'VID',\n",
    "                             'did': 'DID',\n",
    "                             'routeStates.vtime': 'TIME'}) \\\n",
    "            .reindex(['TIME', 'VID', 'LAT', 'LON', 'DID'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the arrivals from the location data, we find local minima for the distance, over time, of a given bus from a given stop. If a bus gets close enough to a given stop, we consider that bus as arriving at that particular stop.\n",
    "\n",
    "There are two implementations presented for computing the distance between a bus and a stop - one uses the `geopy` library to compute the geodesic distance, and the other uses the haversine formula to compute the distance. While the former is more accurate, it can be prohibitively slow to use on large portions of the data set. The latter is faster, but less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haversine formula for calcuating distance between two coordinates in lat lon\n",
    "# from bird eye view; seems to be +- 8 meters difference from geopy distance\n",
    "def haver_distance(latstop,lonstop,latbus,lonbus):\n",
    "\n",
    "    latstop,lonstop,latbus,lonbus = map(np.deg2rad,[latstop,lonstop,latbus,lonbus])\n",
    "    eradius = 6371000\n",
    "    \n",
    "    latdiff = (latbus-latstop)\n",
    "    londiff = (lonbus-lonstop)\n",
    "    \n",
    "    a = np.sin(latdiff/2)**2 + np.cos(latstop)*np.cos(latbus)*np.sin(londiff/2)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    \n",
    "    distance = eradius*c\n",
    "    return distance\n",
    "\n",
    "def find_eclipses(buses, stop):\n",
    "    \"\"\"\n",
    "    Find movement of buses relative to the stop, in distance as a function of time.\n",
    "    \"\"\"\n",
    "    def split_eclipses(eclipses, threshold=30*60*1000) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split buses' movements when they return to a stop after completing the route.\n",
    "        \"\"\"\n",
    "        disjoint_eclipses = []\n",
    "        for bus_id in eclipses['VID'].unique(): # list of unique VID's\n",
    "            # obtain distance data for this one bus\n",
    "            bus = eclipses[eclipses['VID'] == bus_id].sort_values('TIME')\n",
    "            #pprint.pprint(bus)\n",
    "            #pprint.pprint(bus['TIME'].shift())\n",
    "            #pprint.pprint(bus['TIME'].shift() + threshold)\n",
    "            #print('===============')\n",
    "            # split data into groups when there is at least a `threshold`-ms gap between data points\n",
    "            group_ids = (bus['TIME'] > (bus['TIME'].shift() + threshold)).cumsum()\n",
    "\n",
    "            # store groups\n",
    "            for _, group in bus.groupby(group_ids):\n",
    "                disjoint_eclipses.append(group)\n",
    "        return disjoint_eclipses\n",
    "\n",
    "    eclipses = buses.copy()\n",
    "    #eclipses['DIST'] = eclipses.apply(lambda row: distance(stop[['LAT','LON']],row[['LAT','LON']]).meters,axis=1)\n",
    "    \n",
    "    stopcord = stop[['LAT', 'LON']]\n",
    "    buscord = eclipses[['LAT', 'LON']]\n",
    "\n",
    "    # calculate distances fast with haversine function \n",
    "    eclipses['DIST'] = haver_distance(stopcord['LAT'],stopcord['LON'],buscord['LAT'],buscord['LON'])\n",
    "    # only keep positions within 750 meters within the given stop; (filtering out)\n",
    "    eclipses = eclipses[eclipses['DIST'] < 750]\n",
    "    \n",
    "    # update the coordinates list \n",
    "    stopcord = stop[['LAT', 'LON']].values\n",
    "    buscord = eclipses[['LAT', 'LON']].values\n",
    "    \n",
    "    # calculate distances again using geopy for the distance<750m values, because geopy is probably more accurate\n",
    "    dfromstop = []\n",
    "    for row in buscord:\n",
    "        busdistance = distance(stopcord,row).meters\n",
    "        dfromstop.append(busdistance)\n",
    "    eclipses['DIST'] = dfromstop\n",
    "    \n",
    "    # for haversine function:\n",
    "    #stopcord = stop[['LAT', 'LON']]\n",
    "    #buscord = eclipses[['LAT', 'LON']]\n",
    "    #eclipses['DIST'] = haver_distance(stopcord['LAT'],stopcord['LON'],buscord['LAT'],buscord['LON'])\n",
    "    \n",
    "    eclipses['TIME'] = eclipses['TIME'].astype(np.int64)\n",
    "    eclipses = eclipses[['TIME', 'VID', 'DIST']]\n",
    "    \n",
    "    eclipses = split_eclipses(eclipses)\n",
    "    \n",
    "    return eclipses\n",
    "\n",
    "def find_nadirs(eclipses):\n",
    "    \"\"\"\n",
    "    Find points where buses are considered to have encountered the stop.\n",
    "    \n",
    "    Nadir is an astronomical term that describes the lowest point reached by an orbiting body.\n",
    "    \"\"\"\n",
    "    def calc_nadir(eclipse: pd.DataFrame) -> Union[pd.Series, None]:\n",
    "        nadir = eclipse.iloc[eclipse['DIST'].values.argmin()]\n",
    "        if nadir['DIST'] < 100:  # if min dist < 100, then reasonable candidate for nadir\n",
    "            return nadir\n",
    "        else:  # otherwise, hardcore datasci is needed\n",
    "            rev_eclipse = eclipse.iloc[::-1]\n",
    "            rev_nadir = rev_eclipse.iloc[rev_eclipse['DIST'].values.argmin()]\n",
    "            if nadir['TIME'] == rev_nadir['TIME']:  # if eclipse has a global min\n",
    "                return nadir  # then it's the best candidate for nadir\n",
    "            else:  # if eclipse's min occurs at two times\n",
    "                mid_nadir = nadir.copy()\n",
    "                mid_nadir['DIST'] = (nadir['DIST'] + rev_nadir['DIST'])/2\n",
    "                return mid_nadir  # take the midpoint of earliest and latest mins\n",
    "    \n",
    "    nadirs = []\n",
    "    for eclipse in eclipses:\n",
    "        nadirs.append(calc_nadir(eclipse)[['VID', 'TIME']])\n",
    "        \n",
    "    return pd.DataFrame(nadirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stops from a given data set\n",
    "# getting all stops at once from the entire set of location data might take prohibitively long (~3-4 hours)\n",
    "def get_all_stops(data):\n",
    "    bus_stops = pd.DataFrame(columns = [\"VID\", \"TIME\", \"SID\", \"DID\", \"ROUTE\"])\n",
    "    \n",
    "    for route in {ele['rid'] for ele in data}:\n",
    "        print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: Starting with {route}.\")\n",
    "        try:\n",
    "            stop_ids = [stop['id']\n",
    "                for stop\n",
    "                in requests.get(f\"http://restbus.info/api/agencies/sf-muni/routes/{route}\").json()['stops']]\n",
    "                 \n",
    "            route_data = [ele for ele in data if ele['rid'] == route]\n",
    "\n",
    "            for stop_id in stop_ids:\n",
    "                try:\n",
    "                    stops = produce_stops(route_data, route)\n",
    "                except:\n",
    "                    print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: could not produce stops df for {stop_id} on route {route}. Skipping.\")\n",
    "                    break\n",
    "\n",
    "                buses = produce_buses(route_data)\n",
    "\n",
    "                stop = stops[stops['SID'] == stop_id].drop_duplicates().squeeze()\n",
    "\n",
    "                try: \n",
    "                    buses = buses[buses['DID'] == stop['DID']]\n",
    "                except ValueError as err: # accounts for stops with no associated direction\n",
    "                    print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: skipping buses for {stop_id} and {route} due to ValueError: {err}\")\n",
    "                    continue\n",
    "\n",
    "                eclipses = find_eclipses(buses, stop)\n",
    "                nadirs = find_nadirs(eclipses)\n",
    "\n",
    "                try:\n",
    "                    # TODO: refactor\n",
    "                    nadirs[\"TIME\"] = nadirs[\"TIME\"].apply(lambda x: datetime.fromtimestamp(x//1000, timezone(timedelta(hours = -8))))\n",
    "                    #nadirs['DATE'] = nadirs['TIME'].apply(lambda x: x.date())\n",
    "                    #nadirs['TIME'] = nadirs['TIME'].apply(lambda x: x.time())\n",
    "                    nadirs[\"SID\"] = stop_id\n",
    "                    nadirs[\"DID\"] = stop[\"DID\"]\n",
    "                    nadirs[\"ROUTE\"] = route\n",
    "                    bus_stops = bus_stops.append(nadirs, sort = True)\n",
    "                except:\n",
    "                    print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: could not produce stops df for {stop_id} on route {route}. Skipping.\", end = \"\\r\")              \n",
    "        except KeyError:\n",
    "            print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: KeyError at {route}!\")\n",
    "            continue\n",
    "                      \n",
    "    #if len(bus_stops) > 0:\n",
    "    #    bus_stops['timestamp'] = bus_stops[['DATE', 'TIME']].apply(lambda x: datetime.strptime(f\"{x['DATE'].isoformat()} {x['TIME'].isoformat()} -0800\", \n",
    "    #                                                                                   \"%Y-%m-%d %H:%M:%S %z\"), axis = 'columns')\n",
    "    \n",
    "    return bus_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 28 09:47:17 PM: Starting with 14.\n",
      "Thu Feb 28 10:05:42 PM: skipping buses for 37742 and 14 due to ValueError: Can only compare identically-labeled Series objects\n",
      "Thu Feb 28 10:07:16 PM: skipping buses for 7742 and 14 due to ValueError: Can only compare identically-labeled Series objects\n"
     ]
    }
   ],
   "source": [
    "sample_stops = get_all_stops(bus_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_all_stops` produces a DataFrame containing every instance of a bus arriving at a stop from the given data. Columns of note:\n",
    "\n",
    "- *DID*: the direction ID of the bus; the O/I in the string is the most relevant part of this value, as it indicates whether the bus is outbound or inbound.\n",
    "\n",
    "- *SID*: the ID of the stop at which the bus arrived.\n",
    "\n",
    "- *VID*: the vehicle ID of the bus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DID</th>\n",
       "      <th>ROUTE</th>\n",
       "      <th>SID</th>\n",
       "      <th>TIME</th>\n",
       "      <th>VID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121084</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5598</td>\n",
       "      <td>2019-01-15 14:35:36-08:00</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241233</th>\n",
       "      <td>14___I_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5620</td>\n",
       "      <td>2019-01-17 04:43:43-08:00</td>\n",
       "      <td>7293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368542</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5548</td>\n",
       "      <td>2019-01-18 16:10:39-08:00</td>\n",
       "      <td>7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249550</th>\n",
       "      <td>14___I_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5605</td>\n",
       "      <td>2019-01-17 07:05:04-08:00</td>\n",
       "      <td>7224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265166</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5616</td>\n",
       "      <td>2019-01-17 10:47:30-08:00</td>\n",
       "      <td>7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489948</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5607</td>\n",
       "      <td>2019-01-20 15:29:30-08:00</td>\n",
       "      <td>7249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251234</th>\n",
       "      <td>14___I_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5537</td>\n",
       "      <td>2019-01-17 07:30:06-08:00</td>\n",
       "      <td>7224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237039</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5539</td>\n",
       "      <td>2019-01-17 00:59:02-08:00</td>\n",
       "      <td>5433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357710</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5562</td>\n",
       "      <td>2019-01-18 13:42:32-08:00</td>\n",
       "      <td>7268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366753</th>\n",
       "      <td>14___I_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5583</td>\n",
       "      <td>2019-01-18 15:48:08-08:00</td>\n",
       "      <td>7226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DID ROUTE   SID                       TIME   VID\n",
       "121084  14___O_F00    14  5598  2019-01-15 14:35:36-08:00  7244\n",
       "241233  14___I_F00    14  5620  2019-01-17 04:43:43-08:00  7293\n",
       "368542  14___O_F00    14  5548  2019-01-18 16:10:39-08:00  7229\n",
       "249550  14___I_F00    14  5605  2019-01-17 07:05:04-08:00  7224\n",
       "265166  14___O_F00    14  5616  2019-01-17 10:47:30-08:00  7277\n",
       "489948  14___O_F00    14  5607  2019-01-20 15:29:30-08:00  7249\n",
       "251234  14___I_F00    14  5537  2019-01-17 07:30:06-08:00  7224\n",
       "237039  14___O_F00    14  5539  2019-01-17 00:59:02-08:00  5433\n",
       "357710  14___O_F00    14  5562  2019-01-18 13:42:32-08:00  7268\n",
       "366753  14___I_F00    14  5583  2019-01-18 15:48:08-08:00  7226"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_stops.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 69564 entries, 45 to 509886\n",
      "Data columns (total 5 columns):\n",
      "DID      69564 non-null object\n",
      "ROUTE    69564 non-null object\n",
      "SID      69564 non-null object\n",
      "TIME     69564 non-null object\n",
      "VID      69564 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 24.4 MB\n"
     ]
    }
   ],
   "source": [
    "sample_stops.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('route_14_week_stops.json', 'w') as f:\n",
    "    sample_stops.reset_index().drop(labels = 'index', axis = 'columns').to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
